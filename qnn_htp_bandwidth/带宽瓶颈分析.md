# QNN HTP 带宽瓶颈分析

## 当前状态

- **测试结果**: 25.52 GB/s (1GB数据)
- **硬件理论带宽**: 60+ GB/s
- **差距**: 约57%的提升空间

## 问题分析

### 1. 测试方法问题

**当前测试方法**：
```cpp
double start_time = get_time();
for (int i = 0; i < num_iterations; i++) {
    graphExecute(...);  // 同步执行，包含计算时间
}
double end_time = get_time();
bandwidth = total_data / elapsed_time;
```

**问题**：
- 测试时间包含了：**内存传输时间 + 计算时间（Add操作）+ 同步等待时间**
- Element-wise Add虽然计算简单，但仍需要时间
- 无法准确测量纯内存带宽

### 2. 同步执行瓶颈

**graphExecute是同步的**：
- 每次调用都等待HTP完成所有操作
- 无法利用流水线或并行执行
- 包含CPU-HTP通信开销

### 3. 内存访问模式

**当前配置**：
- 使用独立的输入/输出buffer（已优化）
- 使用heapid=25（已优化）
- 使用RAW模式（简单但可能不是最优）

**可能的问题**：
- 内存对齐可能不够优化
- DMA批量传输可能未充分利用
- 内存注册（QnnMem_register）可能提供更好的性能

### 4. 算子选择

**Element-wise Add**：
- 计算量很小（O(n)）
- 可能无法充分利用HTP的计算资源
- 计算时间占比可能较高

## 优化方案

### 方案1: 使用异步执行（如果支持）

**目标**: 减少同步等待时间，利用流水线

**实现**：
- 检查QNN SDK是否支持`graphExecuteAsync`
- 使用回调函数或事件机制
- 批量提交多个执行请求

**预期提升**: 10-20%

### 方案2: 创建纯内存拷贝测试

**目标**: 测量纯内存带宽，排除计算开销

**实现**：
- 优化现有算子的计算效率
- 或使用更简单的数学运算
- 减少计算时间在总时间中的占比

**预期提升**: 20-30%

### 方案3: 优化内存对齐和DMA配置

**目标**: 充分利用DMA批量传输

**实现**：
- 确保内存对齐到64字节或128字节
- 使用更大的传输块
- 配置DMA批量传输参数

**预期提升**: 10-15%

### 方案4: 使用内存注册（QnnMem_register）

**目标**: 优化内存访问路径

**实现**：
- 正确使用`QnnMem_register`注册ION内存
- 使用`QNN_TENSORMEMTYPE_MEMHANDLE`而不是RAW
- 减少内存拷贝和映射开销

**预期提升**: 15-25%

### 方案5: 批量执行和流水线

**目标**: 充分利用HTP资源

**实现**：
- 同时执行多个graph实例
- 使用流水线：准备下一批数据时执行当前批次
- 增加并发度

**预期提升**: 20-30%

### 方案6: 使用计算密集型算子

**目标**: 减少计算时间占比

**实现**：
- 使用卷积、矩阵乘法等计算密集型算子
- 计算时间占比降低，内存带宽占比提高
- 更接近真实应用场景

**预期提升**: 10-20%

## 推荐实施顺序

1. **方案2（纯内存拷贝）** - 最直接，可以验证硬件带宽上限
2. **方案4（内存注册）** - 可能提供显著提升
3. **方案3（内存对齐）** - 相对简单
4. **方案1（异步执行）** - 如果SDK支持
5. **方案5（批量执行）** - 需要更多实现工作
6. **方案6（计算密集型算子）** - 作为对比测试

## 预期总提升

如果所有方案都实施，预期可以达到：
- **保守估计**: 40-50 GB/s
- **理想情况**: 接近硬件理论带宽 60+ GB/s

## 下一步行动

1. 优化算子计算效率
2. 尝试使用QnnMem_register
3. 优化内存对齐
4. 检查异步执行支持
